| Paper | Publication | Author | Year       | Remarks |
|-------|------------|--------|------------|---------|
| [Teex: a toolbox for the evaluation of explanations](https://upcommons.upc.edu/handle/2117/355625) |  Neurocomputing | <ul><li>Jesús M. Antoñanzas</li><li>Yunzhe Jia</li><li>Eibe Frank</li><li>Albert Bifet</li><li>Bernhard Pfahringer</li></ul>| 2023 | <ul><li>A toolbox for evaluating post-hoc feature explainers that repurposes common metrics for machine learning such as precision, recall, F1 score etc for XAI to standardise evaluation.</li><li>Allows XAI attributions to be compared with a ground truth or against eachother. While I'm not convinced the comparing against eachother part is useful, I think the library of ground truth datasets that it provides could be useful.</li><li>Particularly, I would be worried about using the image-based ground truth comparison given that the visual inspection of XAI methods is now known to be bad practice.</li><li>Precision is calculated as the number of non-zero importances are also non-zero in the groun-truth. Whereas, recall is the number of non-zero importances in the groun truth that are also in the explanation. (Although these seem identical to me?!?)</li><li>They have an approach to compare rule-based methods (such as Anchors) with feature explainers (such as LIME) by first converting them into a feature importance vector (which could be useful if we decide we need more explainers in AGREE).</li><li>They cover image, tabular, tabular rules and text-based data.</li></ul>|