| Paper | Publication | Author | Year       | Remarks |
|-------|------------|--------|------------|---------|
| [ILIME: Local and Global Interpretable Model-Agnostic Explainer of Black-Box Decision](https://www.researchgate.net/publication/335437121_ILIME_Local_and_Global_Interpretable_Model-Agnostic_Explainer_of_Black-Box_Decision) | Advances in Databases and Information Systems (ADBIS) | <ul><li>Radwa El Shawi</li><li>Youssef Sherif</li><li>Mouaz H Al-mallah</li><li>Sherif Sakr</li></ul>| 2019 | <ul><li> Improvement on LIME over the original implementation by determining the influence of a perturbed instance on the feature attribution.</li><li>Implements a new global aggregation strategy using hierarchical clustering and BIC to determine the optimal number of local samples to be included.</li><li>Reports that the fidelity of ILIME is far greater than LIME which makes sense due to the reduced reliance on randomness.</li></ul>|
| [DLIME: A Deterministic Local Interpretable Model-Agnostic Explanations Approach for Computer-Aided Diagnosis Systems](https://arxiv.org/abs/1906.10263) | ArXiV | <ul><li>Muhammad Rehman Zafar</li><li>Naimul Mefraz Khan</li></ul>| 2019 | <ul><li> Improvement on LIME over the original implementation by using clustering as opposed to perturbation for training a local linear model.</li><li>Reports a higher stability over repeat iterations, which is a limiting factor of LIME.</li><li>Not peer-reviewed and seems like a shallow evaluation, so take with a pinch of salt.</li></ul>|
| [Explainable Artificial Intelligence for Tabular Data: A Survey](https://ieeexplore.ieee.org/document/9551946) | IEEE Access | <ul><li>Maria Sahakyan</li><li>Zeyar Aung</li><li>Talal Rahwan</li></ul>| 2021 | <ul><li>Useful survey on feature attribution explainers for tabular data.</li></ul>|
| [ALIME: Autoencoder Based Approach for Local Interpretability](https://link.springer.com/chapter/10.1007/978-3-030-33607-3_49) |  Intelligent Data Engineering and Automated Learning (IDEAL) | <ul><li>Sharath M. Shankaranarayana</li><li>Davor Runje</li></ul>| 2019 | <ul><li>An improvement on LIME that uses an autoencoder instead of perturbation to create new samples.</li><li>The distance in the latent space is used as a measure of importance.</li><li>It is shown to be more stable, faster and have a higher local fidelity than LIME.</li></ul>|
| [Do Not Trust Additive Explanations](https://arxiv.org/abs/1903.11420) |  ArXiV | <ul><li>Alicja Gosiewska</li><li>Przemyslaw Biecek</li></ul>| 2019 | <ul><li>A critique paper on additive explainers such as LIME, SHAP and BreakDown stating that they fail to capture interactions between features which is a cause for instability.</li><li>Also propose a framework for measuring the uncertainty of explanations.</li><li>Another useful paper to highlight disagreement as they show that different XAI methods applied to the same prediction can produce conflicting explanations, often with different signs.</li></ul>|
| [Unrestricted permutation forces extrapolation: variable importance requires at least one more model, or there is no free variable importance](https://link.springer.com/article/10.1007/s11222-021-10057-z) |  Statistics and Computing | <ul><li>G Hooker</li><li>L Mentch</li><li>S Zhou</li></ul>| 2021 | <ul><li>Another critique paper that argues that permutation of features for explanation can cause out-of-bound distributions, causing the approximate model (the local linear model for example) to be too far skewed from the original black box, causing low fidelity.</li><li>Again, they touch on the fact that LIME and similar methods don't take into consideration correlated features.</li></ul>|
| [Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead](https://www.nature.com/articles/s42256-019-0048-x) |  Nature Machine Intelligence | <ul><li>Cynthia Rudin</li></ul>| 2019 | <ul><li>A perspective paper from Cynthia Rudin that presented at ICCBR 23 as an invited talk. She proposes that interpretable models are suitable in most applications and should be favoured over black box models - particularly as black box explanations can never reach 100% fidelity or else there would be no need for them.</li><li>However, she does discuss the case that black box models can hide proprietary ML models, so it is unrealistic to assume companies will want to use interpretable models as there is no incentive to. Although, she does push for some tighter policy around the exploration of interpretable models and reporting of accuracy/interpretability trade-off.</li><li>Intrestingly, she shows that there are interpretable models that can produce even image explanations that are more trustworthy than saliency maps as the latter approach can often just highlight edges and where the model is looking rather than class-specific information. As a result, the explanations can be misleading, particularly if not shown with explanations of other classes for context, as if this is done it can show saliency maps are producing similar explanations across all classes.</li></ul>|


