| Paper | Publication | Author | Year       | Remarks |
|-------|------------|--------|------------|---------|
| [ILIME: Local and Global Interpretable Model-Agnostic Explainer of Black-Box Decision](https://www.researchgate.net/publication/335437121_ILIME_Local_and_Global_Interpretable_Model-Agnostic_Explainer_of_Black-Box_Decision) | Advances in Databases and Information Systems (ADBIS) | <ul><li>Radwa El Shawi</li><li>Youssef Sherif</li><li>Mouaz H Al-mallah</li><li>Sherif Sakr</li></ul>| 2019 | <ul><li> Improvement on LIME over the original implementation by determining the influence of a perturbed instance on the feature attribution.</li><li>Implements a new global aggregation strategy using hierarchical clustering and BIC to determine the optimal number of local samples to be included.</li><li>Reports that the fidelity of ILIME is far greater than LIME which makes sense due to the reduced reliance on randomness.</li></ul>|
| [DLIME: A Deterministic Local Interpretable Model-Agnostic Explanations Approach for Computer-Aided Diagnosis Systems](https://arxiv.org/abs/1906.10263) | ArXiV | <ul><li>Muhammad Rehman Zafar</li><li>Naimul Mefraz Khan</li></ul>| 2019 | <ul><li> Improvement on LIME over the original implementation by using clustering as opposed to perturbation for training a local linear model.</li><li>Reports a higher stability over repeat iterations, which is a limiting factor of LIME.</li><li>Not peer-reviewed and seems like a shallow evaluation, so take with a pinch of salt.</li></ul>|
| [Explainable Artificial Intelligence for Tabular Data: A Survey](https://ieeexplore.ieee.org/document/9551946) | IEEE Access | <ul><li>Maria Sahakyan</li><li>Zeyar Aung</li><li>Talal Rahwan</li></ul>| 2021 | <ul><li>Useful survey on feature attribution explainers for tabular data.</li></ul>|
| [ALIME: Autoencoder Based Approach for Local Interpretability](https://link.springer.com/chapter/10.1007/978-3-030-33607-3_49) |  Intelligent Data Engineering and Automated Learning (IDEAL) | <ul><li>Sharath M. Shankaranarayana</li><li>Davor Runje</li></ul>| 2019 | <ul><li>An improvement on LIME that uses an autoencoder instead of perturbation to create new samples.</li><li>The distance in the latent space is used as a measure of importance.</li><li>It is shown to be more stable, faster and have a higher local fidelity than LIME.</li></ul>|
| [Do Not Trust Additive Explanations](https://arxiv.org/abs/1903.11420) |  ArXiV | <ul><li>Alicja Gosiewska</li><li>Przemyslaw Biecek</li></ul>| 2019 | <ul><li>A critique paper on additive explainers such as LIME, SHAP and BreakDown stating that they fail to capture interactions between features which is a cause for instability.</li><li>Also propose a framework for measuring the uncertainty of explanations.</li><li>Another useful paper to highlight disagreement as they show that different XAI methods applied to the same prediction can produce conflicting explanations, often with different signs.</li></ul>|
| [Unrestricted permutation forces extrapolation: variable importance requires at least one more model, or there is no free variable importance](https://link.springer.com/article/10.1007/s11222-021-10057-z) |  Statistics and Computing | <ul><li>G Hooker</li><li>L Mentch</li><li>S Zhou</li></ul>| 2021 | <ul><li>Another critique paper that argues that permutation of features for explanation can cause out-of-bound distributions, causing the approximate model (the local linear model for example) to be too far skewed from the original black box, causing low fidelity.</li><li>Again, they touch on the fact that LIME and similar methods don't take into consideration correlated features.</li></ul>|
| [Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead](https://www.nature.com/articles/s42256-019-0048-x) |  Nature Machine Intelligence | <ul><li>Cynthia Rudin</li></ul>| 2019 | <ul><li>A perspective paper from Cynthia Rudin that presented at ICCBR 23 as an invited talk. She proposes that interpretable models are suitable in most applications and should be favoured over black box models - particularly as black box explanations can never reach 100% fidelity or else there would be no need for them.</li><li>However, she does discuss the case that black box models can hide proprietary ML models, so it is unrealistic to assume companies will want to use interpretable models as there is no incentive to. Although, she does push for some tighter policy around the exploration of interpretable models and reporting of accuracy/interpretability trade-off.</li><li>Intrestingly, she shows that there are interpretable models that can produce even image explanations that are more trustworthy than saliency maps as the latter approach can often just highlight edges and where the model is looking rather than class-specific information. As a result, the explanations can be misleading, particularly if not shown with explanations of other classes for context, as if this is done it can show saliency maps are producing similar explanations across all classes.</li></ul>|
| [The (Un)reliability of Saliency Methods](https://link.springer.com/chapter/10.1007/978-3-030-28954-6_14) |  Explainable AI: Interpreting, Explaining and Visualizing Deep Learning | <ul><li>Pieter-Jan Kindermans</li><li>Sara Hooker</li><li>Julius Adebayo</li><li>Maximilian Alber</li><li>Kristof T. Schütt</li><li>Sven Dähne</li><li>Dumitru Erhan</li><li>Been Kim</li></ul>| 2019 | <ul><li>A critique of saliency-based XAI methods highlighting their unreliability due to changes in the input causing different attributions.</li><li>They propose input shift invariance as a means to determine the reliability of such methods where saliency maps should be identical for the same image in identical models.</li><li>To test input shift invariance they applied a semantically meaningless transformation to the entire dataset (such as adding a checkered box) and updated the bias to accommodate, resulting in an identical model with the same weights and prediction performance. They then tested several saliency-based methods such as Gradients x Input, SmmoothGrad, Integrated Gradients, and Deep Taylor and found that most fail the test in at least some capacity, depending on the domain and context (such as using certain baselines or underlying model architectures).</li><li>They then show how such methods are vulnerable to adversarial attacks by adding some form of noise to the input (they use an image of a hand-drawn cat) to produce different attributions.</li></ul>|
| [Sanity Checks for Saliency Maps](https://proceedings.neurips.cc/paper_files/paper/2018/hash/294a8ed24b1ad22ec2e7efea049b8737-Abstract.html) |  NeurIPS | <ul><li>Julius Adebayo</li><li>Justin Gilmer</li><li>Michael Muelly</li><li>Ian Goodfellow</li><li>Moritz Hardt</li><li>Been Kim</li></ul>| 2018 | <ul><li>A demonstration on how visual inspection is not reliable to test the effectiveness of salience-based explainers (meaning we may need to update the visual evaluation for AGREE inspired by Kenny and Keane).</li><li> written by several authors of the "The (Un)reliablity of Saliency Methods" paper, including Ian Goodfellow who invented the GAN.</li><li>They state that for an explainer to be faithful to the model it should be sensitive to noise in the input. To test this assumption, they check the produced saliency maps from several common methods such as IG, SG, Vanilla Gradients, etc. when randomising labels and model parameters separately and inspecting results visually while comparing with an edge detector such as HOG. They find that several methods produce the same explanation, even when the dataset is completely randomised and shows high similarity with edge detectors (backing up the claims from Rudin).</li><li>They find that gradient-based methods such as Vanilla Gradients (and its SmoothGrad variant) and GradCAM pass the sanity check, however, methods such as IG, Guided Backpropagation and Guided GradCAM fail the test.</li></ul>|
| [Visualizing the Impact of Feature Attribution Baselines](https://distill.pub/2020/attribution-baselines/?utm_source=researcher_app&utm_medium=referral&utm_campaign=RESR_MRKT_Researcher_inbound) |  Distill.Pub | <ul><li>Pascal Sturmfels</li><li>Scott Lundberg</li><li>Su-In Lee</li></ul>| 2020 | <ul><li>This paper explains the importance of selecting a good baseline in path attribution explainers such as Integrated Gradients but highlight the difficulty in doing so and that no perfect solution exists at present. One of the authors includes the inventor of LIME Scott Lunberg.</li><li>Again, this paper criticises the use of visual qualitative evaluation to determine the effectiveness of appraoaches due their habit of highlighting edges.</li><li>They discuss the importance of "missingness" where the baseline should include minimal shared information with the query, meaning that using a constant baseline such as all zeroes or a black image is often insufficient.</li><li>They explain that the maximum distance baseline can be chosen such that a new image that is maximally different from the query but remains in the valid pixel range. Yet, this can still include lots of shared data.</li><li>Blurred images can be used, which improves on missingness but has the tendency to highlight frequent data as important.</li><li>The uniform baseline is one that contains random noise from the distribution or the Gaussian baseline can be used to superimpose the noise over the query. Yet, there is still imperfections in these approaches ase there is still a chance that non-missing or proximal information persists.</li><li>One approach is to average over baselines, which is more intuitive for non-constant baselines and is a similar approach to SmoothGrad. Although this assumes independence of pixels and causes a breakdown in the correlation structure.</li><li>They then explain a top-K ablation test to evaluate how replacing or imputing features effects the predicted output logit and explain that this can be used to choose baselines that causes the most significant change. However, this still has the correlation problem and is a meaningless task to ablate pixels. Instead, they offer a "Mass Center" approach where they find the centre of mass of the explanation, draw a box around it and instead ablate that. Thus, the correlation between features is better maintained.</li><li>However, they state that some in the field disagree with ablation tests as they can test the methods with out-of-distribution data. Similarly, this is the case for using interpolated data.</li><li>They conclude by saying that there is no perfect way to evaluate methods or selecting a baseline yet.</ul>|
| ["Why Should I Trust You?": Explaining the Predictions of Any Classifier (LIME)](https://dl.acm.org/doi/abs/10.1145/2939672.2939778) |  International Conference of Knowledge Discovery and Data Mining (KDD) | <ul><li>
Marco Tulio Ribeiro</li><li>Sameer Singh</li><li>Carlos Guestrin</li></ul>| 2016 | <ul><li>This paper introduces LIME for the first time by Ribeiro, a prominent name in the XAI field. They propose two main contributions LIME - for local explanations; and SP-LIME for global explanations.</li>LIME trains a surrogate "locally faithful" model around the query instance by perturbing the query to create a local neighbourhood and fitting an explainable linear model on it, using the co-efficients as explanations.<li></li><li>SP-LIME aggregates local explanations from LIME to gain a global explanation, but has a module to select instances that are not too similar as to avoid bias in the explanation. This performed better than RP-LIME, which randomly picks local explanations.</li><li>They have a number of evaluation methods, including user studies used to examine if users can identify that a model is untrustworthy based on explanations, simulated user studies to study the the effect of untrustworthy features being removed from the model and assessing fidelity by explaining inherently interpretable models such as decision trees and assessing the recall of important features.</li></ul>|
| [A Unified Approach to Interpreting Model Predictions (SHAP)](https://proceedings.neurips.cc/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html) |  NeurIPS | <ul><li>Scott Lundberg</li><li>Su-in Lee</li></ul>| 2017 | <ul><li>This paper introduces SHAP for the first time by Scott Lundberg, a prominent name in the XAI field. They propose a model agonostic explainer Kernel SHAP and model-specific variants based on other explainers from literature: Deep SHAP, Low-Order SHAP, Max SHAP and Linear SHAP.</li>Shapley values is a concept from game-theory that measures the contribution of each player to an outcome in a co-operative game. They explain that methods currently already exist in literature to calculate Shapley values by altering the number of features in the input with different sampling procedures that often require retraining the model.</li><li>They define the term "additive explainer" and identify existing methods such as LIME, DeepLIFT and Layerwise Relevance Propagation that fall into this category. It is defined as an explanation model that is a linear function of binary variables. Additive explainers have the quality that the outcome can be explained by the summation of all feature contributions.</li><li>Three desirable qualities of explainers are identified 1) Local accuracy: the approximate explanation model should match the output of the original model for the simplified input (i.e. faithfulness); 2) Missingness: Features missing in the original input should have no impact (i.e. interpretability); 3) Consistency: if a model changes so that a features contribution increases or stays the same, then the attribution of that input should not change (i.e. cohesion; the sum of contributions should match the expected value for the explained output).</li><li>Kernel SHAP is model-agnostic and leverages LIME to estimate the Shapley values without retraining the model by using a weighted linear regression.</li><li>Deep SHAP is specific to deep neural networks and uses Deep LIFT to approximate the Shapley values.</li><li>Max SHAP uses a permutation function to calculate the probability that each input will increase the maximum value of every other input.</li><li>Linear SHAP is specific to linear models and uses the model co-efficients directly to estimate the Shapley values, assuming feature independence.</li><li>Low-order SHAP is a variation of Linear SHAP but captures feature interdependencies with an approximation of conditional expectencies.</li><li>The paper claims that all additive models should use Shapley values to estimate attributions.</li>They evaluate it with a user study and find that SHAP better aligns with human intuition.</li></ul>|
| [Model Agnostic Supervised Local Explanations](https://proceedings.neurips.cc/paper_files/paper/2018/hash/b495ce63ede0f4efc9eec62cb947c162-Abstract.html) |  NeurIPS | <ul><li>Gregory Plumb</li><li>Denali Molitor</li><li>Ameet S. Talwalkar</li></ul>| 2018 | <ul><li>A model agnostic method supported by DARPA that is similar to LIME in that it builds a local linear surrogate model to explain predictions by using Random Forest-based approaches to selecting neighbours and their importance. MAPLE can also be used as a self-explaining model.</li><li>While it does not provide global explanations, it can identify "global patterns" that they say other local methods can struggle to detect. These are unusual patterns that are discontinuous and models can make incorrect predictions based on them.</li><li>It uses another method "SILO" for capturing the local neighbourhood. It assigns a weight to each training point based on how frequently that training point appears at the same leaf node as a given point across all trees.</li><li>It uses the same method as DStump for feature importance. DStump decides the importance of a feature based on the reduction in impurity of the label based on when it is split at the root of a trees in the random forest.</li><li>The trained local linear model can then be used for inference, which they state has similar performance to SILO, DStump and Random Forest.</li>It can also be used to find "examplar explanations" to give a nearest-neighbours example based semi-factual explanation.<li>They evaluate the explanations in comparison to LIME using their causal local explanation metric that measures the models ability to predict the value of x' given x (a sample from the distribution centred around x). On 7 datasets from UCI, MAPLE was shown to perform better at self-explanation and black-box explanation.</li></ul>|
| [iNNvestigate Neural Networks!](https://www.jmlr.org/papers/volume20/18-540/18-540.pdf) |  Journal of Machine Learning Research | <ul><li>Maximilian Alber</li><li>Sebastian Lapuschkin</li><li>Philipp Seegerer</li><li>Miriam H¨agele</li><li>Kristof T. Sch¨utt </li><li>Gr´egoire Montavon</li><li>Wojciech Samek</li><li>Klaus-Robert M¨uller</li><li>Sven D¨ahne</li><li>Pieter-Jan Kindermans</li></ul>| 2019 | <ul><li>A Python package that implements many baseline Gradient-based explainers including Smoothgrad, Input x Gradients, Vanilla Gradients, Layerwise Relevance Propagation, Guided Backpropagation, Deep Taylor, DeConvnet, PatternNet, PatternAttribution (and later GradCAM and GradCAM++ not discussed in the paper).</li><li>One of the authors is prominent in the field (Wojciech Samek).</li><li>It says other methods can be applied to Smoothgrad for smoothing for a sharper image.</li><li>Includes a permutation analyser to analyse the robustness of image explanations.</li></ul>|




