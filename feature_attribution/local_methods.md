| Paper | Publication | Author | Year       | Remarks |
|-------|------------|--------|------------|---------|
| [ILIME: Local and Global Interpretable Model-Agnostic Explainer of Black-Box Decision](https://www.researchgate.net/publication/335437121_ILIME_Local_and_Global_Interpretable_Model-Agnostic_Explainer_of_Black-Box_Decision) | Advances in Databases and Information Systems (ADBIS) | <ul><li>Radwa El Shawi</li><li>Youssef Sherif</li><li>Mouaz H Al-mallah</li><li>Sherif Sakr</li></ul>| 2019 | <ul><li> Improvement on LIME over the original implementation by determining the influence of a perturbed instance on the feature attribution.</li><li>Implements a new global aggregation strategy using hierarchical clustering and BIC to determine the optimal number of local samples to be included.</li><li>Reports that the fidelity of ILIME is far greater than LIME which makes sense due to the reduced reliance on randomness.</li></ul>|
| [DLIME: A Deterministic Local Interpretable Model-Agnostic Explanations Approach for Computer-Aided Diagnosis Systems](https://arxiv.org/abs/1906.10263) | ArXiV | <ul><li>Muhammad Rehman Zafar</li><li>Naimul Mefraz Khan</li></ul>| 2019 | <ul><li> Improvement on LIME over the original implementation by using clustering as opposed to perturbation for training a local linear model.</li><li>Reports a higher stability over repeat iterations, which is a limiting factor of LIME.</li><li>Not peer-reviewed and seems like a shallow evaluation, so take with a pinch of salt.</li></ul>|
| [Explainable Artificial Intelligence for Tabular Data: A Survey](https://ieeexplore.ieee.org/document/9551946) | IEEE Access | <ul><li>Maria Sahakyan</li><li>Zeyar Aung</li><li>Talal Rahwan</li></ul>| 2021 | <ul><li>Useful survey on feature attribution explainers for tabular data.</li></ul>|
| [ALIME: Autoencoder Based Approach for Local Interpretability](https://link.springer.com/chapter/10.1007/978-3-030-33607-3_49) |  Intelligent Data Engineering and Automated Learning (IDEAL) | <ul><li>Sharath M. Shankaranarayana</li><li>Davor Runje</li></ul>| 2019 | <ul><li>An improvement on LIME that uses an autoencoder instead of perturbation to create new samples.</li><li>The distance in the latent space is used as a measure of importance.</li><li>It is shown to be more stable, faster and have a higher local fidelity than LIME.</li></ul>|
| [Do Not Trust Additive Explanations](https://arxiv.org/abs/1903.11420) |  ArXiV | <ul><li>Alicja Gosiewska</li><li>Przemyslaw Biecek</li></ul>| 2019 | <ul><li>A critique paper on additive explainers such as LIME, SHAP and BreakDown stating that they fail to capture interactions between features which is a cause for instability.</li><li>Also propose a framework for measuring the uncertainty of explanations.</li><li>Another useful paper to highlight disagreement as they show that different XAI methods applied to the same prediction can produce conflicting explanations, often with different signs.</li></ul>|
| [Unrestricted permutation forces extrapolation: variable importance requires at least one more model, or there is no free variable importance](https://link.springer.com/article/10.1007/s11222-021-10057-z) |  Statistics and Computing | <ul><li>G Hooker</li><li>L Mentch</li><li>S Zhou</li></ul>| 2021 | <ul><li>Another critique paper that argues that permutation of features for explanation can cause out-of-bound distributions, causing the approximate model (the local linear model for example) to be too far skewed from the original black box, causing low fidelity.</li><li>Again, they touch on the fact that LIME and similar methods don't take into consideration correlated features.</li></ul>|


