| Paper | Publication | Author | Year       | Remarks |
|-------|------------|--------|------------|---------|
| [ILIME: Local and Global Interpretable Model-Agnostic Explainer of Black-Box Decision](https://www.researchgate.net/publication/335437121_ILIME_Local_and_Global_Interpretable_Model-Agnostic_Explainer_of_Black-Box_Decision) | Advances in Databases and Information Systems (ADBIS) | <ul><li>Radwa El Shawi</li><li>Youssef Sherif</li><li>Mouaz H Al-mallah</li><li>Sherif Sakr</li></ul>| 2019 | <ul><li> Improvement on LIME over the original implementation by determining the influence of a perturbed instance on the feature attribution.</li><li>Implements a new global aggregation strategy using hierarchical clustering and BIC to determine the optimal number of local samples to be included.</li><li>Reports that the fidelity of ILIME is far greater than LIME which makes sense due to the reduced reliance on randomness.</li></ul>|
| [DLIME: A Deterministic Local Interpretable Model-Agnostic Explanations Approach for Computer-Aided Diagnosis Systems](https://arxiv.org/abs/1906.10263) | ArXiV | <ul><li>Muhammad Rehman Zafar</li><li>Naimul Mefraz Khan</li></ul>| 2019 | <ul><li> Improvement on LIME over the original implementation by using clustering as opposed to perturbation for training a local linear model.</li><li>Reports a higher stability over repeat iterations, which is a limiting factor of LIME.</li><li>Not peer-reviewed and seems like a shallow evaluation, so take with a pinch of salt.</li></ul>|
| [Explainable Artificial Intelligence for Tabular Data: A Survey](https://ieeexplore.ieee.org/document/9551946) | IEEE Access | <ul><li>Maria Sahakyan</li><li>Zeyar Aung</li><li>Talal Rahwan</li></ul>| 2021 | <ul><li>Useful survey on feature attribution explainers for tabular data.</li></ul>|
| [ALIME: Autoencoder Based Approach for Local Interpretability](https://link.springer.com/chapter/10.1007/978-3-030-33607-3_49) |  Intelligent Data Engineering and Automated Learning (IDEAL) | <ul><li>Sharath M. Shankaranarayana</li><li>Davor Runje</li></ul>| 2019 | <ul><li>An improvement on LIME that uses an autoencoder instead of perturbation to create new samples.</li><li>The distance in the latent space is used as a measure of importance.</li><li>It is shown to be more stable, faster and have a higher local fidelity than LIME.</li></ul>|
| [Do Not Trust Additive Explanations](https://arxiv.org/abs/1903.11420) |  ArXiV | <ul><li>Alicja Gosiewska</li><li>Przemyslaw Biecek</li></ul>| 2019 | <ul><li>A critique paper on additive explainers such as LIME, SHAP and BreakDown stating that they fail to capture interactions between features which is a cause for instability.</li><li>Also propose a framework for measuring the uncertainty of explanations.</li><li>Another useful paper to highlight disagreement as they show that different XAI methods applied to the same prediction can produce conflicting explanations, often with different signs.</li></ul>|
| [Unrestricted permutation forces extrapolation: variable importance requires at least one more model, or there is no free variable importance](https://link.springer.com/article/10.1007/s11222-021-10057-z) |  Statistics and Computing | <ul><li>G Hooker</li><li>L Mentch</li><li>S Zhou</li></ul>| 2021 | <ul><li>Another critique paper that argues that permutation of features for explanation can cause out-of-bound distributions, causing the approximate model (the local linear model for example) to be too far skewed from the original black box, causing low fidelity.</li><li>Again, they touch on the fact that LIME and similar methods don't take into consideration correlated features.</li></ul>|
| [Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead](https://www.nature.com/articles/s42256-019-0048-x) |  Nature Machine Intelligence | <ul><li>Cynthia Rudin</li></ul>| 2019 | <ul><li>A perspective paper from Cynthia Rudin that presented at ICCBR 23 as an invited talk. She proposes that interpretable models are suitable in most applications and should be favoured over black box models - particularly as black box explanations can never reach 100% fidelity or else there would be no need for them.</li><li>However, she does discuss the case that black box models can hide proprietary ML models, so it is unrealistic to assume companies will want to use interpretable models as there is no incentive to. Although, she does push for some tighter policy around the exploration of interpretable models and reporting of accuracy/interpretability trade-off.</li><li>Intrestingly, she shows that there are interpretable models that can produce even image explanations that are more trustworthy than saliency maps as the latter approach can often just highlight edges and where the model is looking rather than class-specific information. As a result, the explanations can be misleading, particularly if not shown with explanations of other classes for context, as if this is done it can show saliency maps are producing similar explanations across all classes.</li></ul>|
| [The (Un)reliability of Saliency Methods](https://link.springer.com/chapter/10.1007/978-3-030-28954-6_14) |  Explainable AI: Interpreting, Explaining and Visualizing Deep Learning | <ul><li>Pieter-Jan Kindermans</li><li>Sara Hooker</li><li>Julius Adebayo</li><li>Maximilian Alber</li><li>Kristof T. Schütt</li><li>Sven Dähne</li><li>Dumitru Erhan</li><li>Been Kim</li></ul>| 2019 | <ul><li>A critique of saliency-based XAI methods highlighting their unreliability due to changes in the input causing different attributions.</li><li>They propose input shift invariance as a means to determine the reliability of such methods where saliency maps should be identical for the same image in identical models.</li><li>To test input shift invariance they applied a semantically meaningless transformation to the entire dataset (such as adding a checkered box) and updated the bias to accommodate, resulting in an identical model with the same weights and prediction performance. They then tested several saliency-based methods such as Gradients x Input, SmmoothGrad, Integrated Gradients, and Deep Taylor and found that most fail the test in at least some capacity, depending on the domain and context (such as using certain baselines or underlying model architectures).</li><li>They then show how such methods are vulnerable to adversarial attacks by adding some form of noise to the input (they use an image of a hand-drawn cat) to produce different attributions.</li></ul>|
| [Sanity Checks for Saliency Maps](https://proceedings.neurips.cc/paper_files/paper/2018/hash/294a8ed24b1ad22ec2e7efea049b8737-Abstract.html) |  NeurIPS | <ul><li>Julius Adebayo</li><li>Justin Gilmer</li><li>Michael Muelly</li><li>Ian Goodfellow</li><li>Moritz Hardt</li><li>Been Kim</li></ul>| 2018 | <ul><li>A demonstration on how visual inspection is not reliable to test the effectiveness of salience-based explainers (meaning we may need to update the visual evaluation for AGREE inspired by Kenny and Keane).</li><li> written by several authors of the "The (Un)reliablity of Saliency Methods" paper, including Ian Goodfellow who invented the GAN.</li><li>They state that for an explainer to be faithful to the model it should be sensitive to noise in the input. To test this assumption, they check the produced saliency maps from several common methods such as IG, SG, Vanilla Gradients, etc. when randomising labels and model parameters separately and inspecting results visually while comparing with an edge detector such as HOG. They find that several methods produce the same explanation, even when the dataset is completely randomised and shows high similarity with edge detectors (backing up the claims from Rudin).</li><li>They find that gradient-based methods such as Vanilla Gradients (and its SmoothGrad variant) and GradCAM pass the sanity check, however, methods such as IG, Guided Backpropagation and Guided GradCAM fail the test.</li></ul>|
| [Visualizing the Impact of Feature Attribution Baselines](https://distill.pub/2020/attribution-baselines/?utm_source=researcher_app&utm_medium=referral&utm_campaign=RESR_MRKT_Researcher_inbound) |  Distill.Pub | <ul><li>Pascal Sturmfels</li><li>Scott Lundberg</li><li>Su-In Lee</li></ul>| 2020 | <ul><li>This paper explains the importance of selecting a good baseline in path attribution explainers such as Integrated Gradients but highlight the difficulty in doing so and that no perfect solution exists at present. One of the authors includes the inventor of LIME Scott Lunberg.</li><li>Again, this paper criticises the use of visual qualitative evaluation to determine the effectiveness of appraoaches due their habit of highlighting edges.</li><li>They discuss the importance of "missingness" where the baseline should include minimal shared information with the query, meaning that using a constant baseline such as all zeroes or a black image is often insufficient.</li><li>They explain that the maximum distance baseline can be chosen such that a new image that is maximally different from the query but remains in the valid pixel range. Yet, this can still include lots of shared data.</li><li>Blurred images can be used, which improves on missingness but has the tendency to highlight frequent data as important.</li><li>The uniform baseline is one that contains random noise from the distribution or the Gaussian baseline can be used to superimpose the noise over the query. Yet, there is still imperfections in these approaches ase there is still a chance that non-missing or proximal information persists.</li><li>One approach is to average over baselines, which is more intuitive for non-constant baselines and is a similar approach to SmoothGrad. Although this assumes independence of pixels and causes a breakdown in the correlation structure.</li><li>They then explain a top-K ablation test to evaluate how replacing or imputing features effects the predicted output logit and explain that this can be used to choose baselines that causes the most significant change. However, this still has the correlation problem and is a meaningless task to ablate pixels. Instead, they offer a "Mass Center" approach where they find the centre of mass of the explanation, draw a box around it and instead ablate that. Thus, the correlation between features is better maintained.</li><li>However, they state that some in the field disagree with ablation tests as they can test the methods with out-of-distribution data. Similarly, this is the case for using interpolated data.</li><li>They conclude by saying that there is no perfect way to evaluate methods or selecting a baseline yet.</ul>|


